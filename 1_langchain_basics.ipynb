{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r ./requirements.txt -q\n",
    "# !pip - installs in base environment\n",
    "# !\\pip - installs in virtual environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.0.341\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /Users/chidi/Library/Python/3.9/lib/python/site-packages\n",
      "Requires: aiohttp, anyio, async-timeout, dataclasses-json, jsonpatch, langchain-core, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain --upgrade -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "# load_dotenv('./.env')\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'091b67e9-39dd-4c7e-aff2-38668b522b6c'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ.get('PINECONE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM Models (Wrappers): GPT-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOpenAI\u001b[0m\n",
      "Params: {'model_name': 'text-davinci-003', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 512}\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model_name='text-davinci-003', temperature=0.7, max_tokens=512)\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Bottom barrel refers to the lowest quality or worst of a given range of items.\n"
     ]
    }
   ],
   "source": [
    "output = llm('explain the meaning of \"bottom barrel\" in one sentence')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(llm.get_num_tokens('explain the meaning of \"bottom barrel\" in one sentence'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm.generate(['Antanarivo is the capital of ...', \n",
    "                   'What is the formula for the perimeter of a sphere?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Generation(text='\\n\\nMadagascar.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nThe formula for the perimeter of a sphere is 2πr, where r is the radius of the sphere.', generation_info={'finish_reason': 'stop', 'logprobs': None})]]\n"
     ]
    }
   ],
   "source": [
    "print(output.generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Madagascar.\n"
     ]
    }
   ],
   "source": [
    "print(output.generations[0][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output.generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm.generate(['Write an original tagline for a burger restaurant']*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Burger Bliss - Satisfy Your Cravings!\"\n",
      "\n",
      "\n",
      "\"Sink your teeth into the juiciest burgers around!\"\n",
      "\n",
      "\n",
      "\"Come for the Burgers, Stay for the Fries!\"\n"
     ]
    }
   ],
   "source": [
    "for o in output.generations:\n",
    "    print(o[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ChatModels: GPT-3.5-Turbo and GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import(\n",
    "    AIMessage, # Assistant message of OpenAI API\n",
    "    HumanMessage, # User message of OpenAI API\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5, max_tokens=1024)\n",
    "messages = [\n",
    "    SystemMessage(content='You are a polyglot and respond only in Spanish.'),\n",
    "    HumanMessage(content='What is \"My Heart\" in your language?')\n",
    "]\n",
    "output = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Mi corazón\" is the translation of \"My Heart\" in Spanish.\n"
     ]
    }
   ],
   "source": [
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['language', 'virus'] template='You are an experienced virologist.\\nWrite a few sentences about the following {virus} in {language}.'\n"
     ]
    }
   ],
   "source": [
    "template = '''You are an experienced virologist.\n",
    "Write a few sentences about the following {virus} in {language}.'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['virus', 'language'],\n",
    "    template=template\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "El ébola es un virus mortal que se transmite a través de fluidos corporales. Los síntomas incluyen fiebre, dolor de cabeza, fatiga y vómitos. Puede ser fatal si no se trata adecuadamente. Es importante que las personas tomen precauciones para evitar la propagación del virus.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name='text-davinci-003', temperature=0.7)\n",
    "output = llm(prompt.format(virus='ebola', language='spanish'))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5)\n",
    "template = '''You are an experienced virologist.\n",
    "Write a few sentences about the following {virus} in {language}.'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['virus', 'language'],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "# output = chain('Staph')\n",
    "output = chain.run({'virus':'HSV', 'language':'french'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le HSV (Herpes Simplex Virus) est un virus de la famille des herpèsviridés qui infecte les êtres humains. Il existe deux types de HSV : le HSV-1 et le HSV-2. Le HSV-1 est principalement responsable des infections buccales, telles que les boutons de fièvre, tandis que le HSV-2 est généralement associé aux infections génitales. Ces virus sont très contagieux et se transmettent par contact direct avec les lésions cutanées ou les muqueuses infectées. Bien qu'ils ne puissent pas être guéris, les symptômes peuvent être traités avec des médicaments antiviraux pour réduire la fréquence et la gravité des poussées.\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m.\n",
      "\n",
      "def softmax(x):\n",
      "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
      "    return np.exp(x) / np.sum(np.exp(x), axis=0)\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mThe Python function softmax() computes the softmax values for each set of scores in x. It takes in an array or matrix of scores as input. \n",
      "\n",
      "The function uses the numpy library to perform mathematical operations. \n",
      "\n",
      "The softmax calculation is achieved by exponentiating each element in the input array (using np.exp(x)) and then dividing the resulting array by the sum of all exponentiated values along the specified axis (using np.sum(np.exp(x), axis=0)). \n",
      "\n",
      "The axis parameter determines the axis along which the sum is performed. In this case, axis=0 indicates that the sum is performed vertically, for each column of the input matrix. \n",
      "\n",
      "Finally, the function returns the softmax values, which are the resulting array obtained from dividing each exponentiated value by the sum.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "llm1 = OpenAI(model_name='text-davinci-003', temperature=0.7, max_tokens=1024)\n",
    "prompt1 = PromptTemplate(\n",
    "    input_variables=['concept'],\n",
    "    template='''You are an experienced scientist and Python programmer.\n",
    "    Write a function that implements the concept of {concept}'''\n",
    ")\n",
    "chain1 = LLMChain(llm=llm1, prompt=prompt1)\n",
    "\n",
    "\n",
    "llm2 = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1.2)\n",
    "prompt2 = PromptTemplate(\n",
    "    input_variables=['function'],\n",
    "    template='Given the Python function {function}, describe it as detailed as possible.'\n",
    ")\n",
    "\n",
    "chain2 = LLMChain(llm=llm2, prompt=prompt2)\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
    "output = overall_chain.run('softmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LangChain Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146306.05007233328"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5.1**7.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install langchain_experimental -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m I need to calculate the factorial of 20 and then take the square root\n",
      "Action: Python_REPL\n",
      "Action Input: from math import factorial\n",
      "               print(round(math.sqrt(factorial(20)), 4))\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIndentationError('unexpected indent', ('<string>', 2, 15, '               print(round(math.sqrt(factorial(20)), 4))\\n'))\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to remove the extra indentation\n",
      "Action: Python_REPL\n",
      "Action Input: from math import factorial\n",
      "print(round(math.sqrt(factorial(20)), 4))\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mNameError(\"name 'math' is not defined\")\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to import the math module\n",
      "Action: Python_REPL\n",
      "Action Input: import math\n",
      "from math import factorial\n",
      "print(round(math.sqrt(factorial(20)), 4))\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m1559776268.6285\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 1559776268.6285\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1559776268.6285'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "agent_executor = create_python_agent(\n",
    "    llm=llm, \n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent_executor.run('Calculate the square root of the factorial of 20 \\\n",
    "                   and display it with 4 decimal points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1559776268.6285"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import factorial\n",
    "\n",
    "round(factorial(20)**0.5,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to use the Python REPL to calculate this\n",
      "Action: Python_REPL\n",
      "Action Input: print(5.1 ** 7.3)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m146306.05007233328\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 146306.05007233328\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'146306.05007233328'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.run('what is the answer to 5.1 ** 7.3?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
